# Dense2MoE: Dense-to-MoE Transformation Pipeline

Dense ëª¨ë¸ì„ Mixture-of-Experts (MoE) ëª¨ë¸ë¡œ ë³€í™˜í•˜ëŠ” í†µí•© íŒŒì´í”„ë¼ì¸

## TL;DR

Dense LLM ëª¨ë¸ ê¸°ë°˜ ë„ë©”ì¸ë³„ LoRA ì–´ëŒ‘í„°ë¥¼ í›ˆë ¨í•˜ê³ , ì´ë¥¼ **Layer-wise MoE ì•„í‚¤í…ì²˜**ë¡œ í†µí•©í•˜ëŠ” End-to-End íŒŒì´í”„ë¼ì¸

### Main Features
- **Multi-Model ì§€ì›**: Llama, Qwen ë“± ë‹¤ì–‘í•œ Dense ëª¨ë¸ ì§€ì›
- **Layer-wise MoE**: ê° layerë§ˆë‹¤ ë…ë¦½ì ì¸ Router
- **Nê°œ Expert**: Nê°œ ë„ë©”ì¸ ì „ë¬¸ê°€ + 1ê°œ zero ì „ë¬¸ê°€
- **ìœ ì—°í•œ í•™ìŠµ**: Routerë§Œ í•™ìŠµ, Router+MLP í•™ìŠµ, Attention í•™ìŠµ ë“± ë‹¤ì–‘í•œ ì˜µì…˜
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: LoRAë¥¼ í†µí•œ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  êµ¬ì¡°

## MoE Architecture

### **Layer Structure**
```
Layer N:
â”œâ”€â”€ Self-Attention (freeze/trainable)
â”œâ”€â”€ Router N (trainable) â†’ N+1ê°œ ì¤‘ Top-K selection
â””â”€â”€ Expert Selection
    â”œâ”€â”€ Domain 1 Expert (LoRA MLP)
    â”œâ”€â”€ Domain 2 Expert (LoRA MLP)
    â”œâ”€â”€ ...
    â””â”€â”€ Zero Expert (ì›ë³¸ FFN)

Output = Î£(Router Weight Ã— Expert Output)
```

### **Overall Architecture**
- **Base Model**: ì„¤ì • ê°€ëŠ¥ (Llama, Qwen ë“±)
- **Experts per Layer**: N+1ê°œ (Nê°œ ë„ë©”ì¸ + 1ê°œ zero)
- **Gating Strategy**: Top-1 (Hard) / Top-2+ (Soft) ì„ íƒ ê°€ëŠ¥

## File Structure

```
Dense2MoE/
â”œâ”€â”€ src/                      # ğŸ—ï¸ í•µì‹¬ ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ models/               # ğŸ¤– MoE ëª¨ë¸ êµ¬í˜„ì²´
â”‚   â”‚   â””â”€â”€ model.py          # MoEModel, LayerRouter, ExpertFFN
â”‚   â”œâ”€â”€ core/                 # ğŸ”§ í•µì‹¬ ì»´í¬ë„ŒíŠ¸
â”‚   â”‚   â”œâ”€â”€ trainer.py        # ë„ë©”ì¸/ë¼ìš°í„° í›ˆë ¨ê¸°
â”‚   â”‚   â”œâ”€â”€ evaluator.py      # ë¼ìš°í„° í‰ê°€ê¸°
â”‚   â”‚   â””â”€â”€ dataset.py        # ë°ì´í„°ì…‹ ì²˜ë¦¬
â”‚   â””â”€â”€ utils/                # ğŸ› ï¸ ìœ í‹¸ë¦¬í‹°
â”‚       â”œâ”€â”€ utils.py          # GPU, ë¡œê¹…, í™˜ê²½ì„¤ì •
â”‚       â””â”€â”€ wandb_utils.py    # WandB ì—°ë™
â”œâ”€â”€ config/                   # âš™ï¸ ì„¤ì • ê´€ë¦¬
â”‚   â”œâ”€â”€ config.yaml           # ë©”ì¸ ì„¤ì • (model.typeìœ¼ë¡œ ëª¨ë¸ ì „í™˜)
â”‚   â”œâ”€â”€ moe.py                # MoE/ëª¨ë¸ ì„¤ì • (MODEL_REGISTRY)
â”‚   â””â”€â”€ domains.py            # ë„ë©”ì¸ ì„¤ì •
â”œâ”€â”€ scripts/                  # ğŸš€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ train_domain.py   # ë„ë©”ì¸ë³„ LoRA í•™ìŠµ
â”‚   â”‚   â””â”€â”€ train_router.py   # MoE ë¼ìš°í„° í•™ìŠµ
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ evaluate.py       # MoE í‰ê°€
â”‚       â””â”€â”€ evaluate_domain.py# ë„ë©”ì¸ LoRA í‰ê°€
â”œâ”€â”€ data/                     # ğŸ“Š ë°ì´í„°
â”œâ”€â”€ domain_models/            # ğŸ¯ í›ˆë ¨ëœ LoRA ì–´ëŒ‘í„°
â”œâ”€â”€ moe_models/               # ğŸ¤– í•™ìŠµëœ ë¼ìš°í„° ì²´í¬í¬ì¸íŠ¸
â””â”€â”€ requirements.txt          # ğŸ“¦ ì˜ì¡´ì„±
```

## Setting

### Model Switching (config.yaml)
```yaml
model:
  type: "llama"  # "llama" ë˜ëŠ” "qwen"
  name: null     # nullì´ë©´ typeì—ì„œ ìë™ ê²°ì •
```

### Supported Models (config/moe.py)
```python
MODEL_REGISTRY = {
    "llama": {"name": "meta-llama/Llama-3.2-3B-Instruct", "num_layers": 28},
    "qwen": {"name": "Qwen/Qwen3-4B-Instruct-2507", "num_layers": 36},
}
```

## Usage

### 1. Train Domain Lora Adapter

```bash
# ì˜ë£Œ ë„ë©”ì¸ í›ˆë ¨
python scripts/training/train_domain.py --domain medical --max-samples 1000

# ë²•ë¥  ë„ë©”ì¸ í›ˆë ¨
python scripts/training/train_domain.py --domain law --max-samples 1000
```

**Main arguments:**
- `--domain`: í•™ìŠµ ë„ë©”ì¸ (medical, law, math, code)
- `--max-samples`: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
- `--epochs`: ì—í­ ìˆ˜
- `--output-dir`: ì €ì¥ ê²½ë¡œ

### 2. Train MoE Router

```bash
# ë¼ìš°í„° í•™ìŠµ (Routerë§Œ)
python scripts/training/train_router.py --output-dir moe_models/run1 --target router

# ë¼ìš°í„° + MLP í•™ìŠµ
python scripts/training/train_router.py --output-dir moe_models/run1 --target router_mlp

# Attention í¬í•¨ í•™ìŠµ
python scripts/training/train_router.py --output-dir moe_models/run1 --target attention
```

**ì£¼ìš” ì˜µì…˜:**
- `--target`: í•™ìŠµ ëŒ€ìƒ (`router`, `mlp`, `attention`, `router_mlp`)
- `--top-k`: Expert ì„ íƒ ìˆ˜ (1: Hard, 2+: Soft routing)
- `--load-balancing-loss-weight`: ë¡œë“œ ë°¸ëŸ°ì‹± ì†ì‹¤ ê°€ì¤‘ì¹˜
- `--use-wandb`: WandB ë¡œê¹…

### 3. Evaluation

```bash
# MoE ëª¨ë¸ í‰ê°€
python scripts/evaluation/evaluate.py \
    --moe-model-path /data/disk5/internship_disk/gyubin/MoE_models/Llama_model/final_router/router_epoch3_12600_aux_0_5e-4_top1/final_model/pytorch_model.bin \
    --domain medical --max-samples 200

# ë„ë©”ì¸ LoRA í‰ê°€
python scripts/evaluation/evaluate_domain.py --domain medical --max-samples 200
```

## Domain Datasets

| ë„ë©”ì¸ | ë°ì´í„°ì…‹ | ì„ íƒì§€ | Train | Test |
|--------|----------|--------|-------|------|
| Medical | MedMCQA | 4 | 20,000 | 1,000 |
| Law | casehold | 5 | 20,000 | 1,000 |
| Math | mathqa | 5 | 20,000 | 1,000 |
| Code | coding-mcq-reasoning | 4 | 3,000 | 300 |
| MMLU | MMLU | 4 | - | 1,000 |

## Pipeline

```bash
# 1. í™˜ê²½ í™œì„±í™”
conda activate moe

# 2. ëª¨ë“  ë„ë©”ì¸ LoRA í›ˆë ¨
for domain in medical law math code; do
    python scripts/training/train_domain.py --domain $domain --epochs 2
done

# 3. MoE ë¼ìš°í„° í›ˆë ¨
python scripts/training/train_router.py --output-dir moe_models/run1 --epochs 1

# 4. í‰ê°€
python scripts/evaluation/evaluate.py \
    --moe-model-path /data/disk5/internship_disk/gyubin/MoE_models/Llama_model/final_router/router_epoch3_12600_aux_0_5e-4_top1/final_model/pytorch_model.bin \
    --domain medical
```

## Results directory

- `domain_models/<domain>/`: ë„ë©”ì¸ë³„ LoRA ì–´ëŒ‘í„°
- `moe_models/<run>/final_model/`: í•™ìŠµëœ ë¼ìš°í„°
- `*_training_summary.json`: í•™ìŠµ ìš”ì•½

---


# Model ìœ„ì¹˜

ì „ì²´ ëª¨ë¸ ë””ë ‰í† ë¦¬: /data/disk5/internship_disk/gyubin/MoE_models
Llama best model
top-1
/data/disk5/internship_disk/gyubin/MoE_models/Llama_model/final_router/router_epoch3_12600_aux_0_5e-4_top1/final_model/pytorch_model.bin
top-2
/data/disk5/internship_disk/gyubin/MoE_models/Llama_model/final_router/router_epoch3_12600_aux_0_2e-4_top2/final_model/pytorch_model.bin


Qwen best model
top-1
/data/disk5/internship_disk/gyubin/MoE_models/Qwen_MoE/final_router/router_epoch3_12600_noaux_5e4_top1/final_model/pytorch_model.bin
top-2
/data/disk5/internship_disk/gyubin/MoE_models/Qwen_MoE/final_router/router_epoch3_12600_noaux_2e4_top2/final_model/pytorch_model.bin





# Experiment Results

# Qwen
# Dense model

- base model
    - í•™ìŠµí•˜ì§€ ì•Šì€ base model
        
        
        | domain | code | law | math | medical | mmlu | average |
        | --- | --- | --- | --- | --- | --- | --- |
        | qwen-base | 69.33 | 66.5 | 40.5 | 61.6 | 72.8 | 62.146 |
- domain model
    - ê° ë„ë©”ì¸ë³„ë¡œ í•™ìŠµí•œ ëª¨ë¸
        
        
        | domain | code | law | math | medical | mmlu | average |
        | --- | --- | --- | --- | --- | --- | --- |
        | **eval-adapter-law** | 63 | 89.6 | 40.5 | 61.3 | 70.3 | **64.94** |
        | **eval-adapter-math** | 69 | 68.9 | 66 | 64.1 | 73.4 | **68.28** |
        | **eval-adapter-medical** | 69.33 | 68.6 | 46.1 | 68.8 | 73.5 | **65.266** |
        | **eval-adapter-code** | 75 | 69.1 | 45.1 | 63.5 | 73.7 | **65.28** |
    - 20%ë§Œ ì‚¬ìš©í•œ ëª¨ë¸
        
        
        | domain | code | law | math | medical | mmlu | average |
        | --- | --- | --- | --- | --- | --- | --- |
        | **eval-qwen-law-only** | 67 | 77.6 | 42.4 | 63.1 | 74.8 | **64.98** |
        | **eval-qwen-math-only** | 68 | 67.7 | 49 | 62 | 74.1 | **64.16** |
        | **eval-qwen-medical-only** | 70 | 68.9 | 44.6 | 64.8 | 75.6 | **64.78** |
        | **eval-qwen-code-only** | 70.33 | 66.8 | 41.8 | 61.5 | 72.9 | **62.666** |
    
- general model
    
    ëª¨ë“  ë°ì´í„°ì…‹ì„ í•™ìŠµí•œ ëª¨ë¸
    
    - mlpë§Œ í•™ìŠµ
        
        
        | name | code | law | math | medical | mmlu | average |
        | --- | --- | --- | --- | --- | --- | --- |
        | **eval-adapter-general** | 76.33 | 87.6 | 63.1 | 69.2 | 74.4 | **74.126** |
    - mlp + attentionë„ í•™ìŠµ
        
        
        | lr | code | law | math | medical | mmlu | average |
        | --- | --- | --- | --- | --- | --- | --- |
        | 2e-4 | 74.33 | 88.0 | 65.8 | 69.7 | 74.5 | 74.466 |
    - 12,600ê°œë¡œ í•™ìŠµ
        
        ëª¨ë“  ë°ì´í„°ì…‹ì„ í•™ìŠµí•œ ëª¨ë¸
        
        - mlpë§Œ í•™ìŠµ
            
            
            | name | code | law | math | medical | mmlu | average |
            | --- | --- | --- | --- | --- | --- | --- |
            | **eval-adapter-general** | 76.33 | 87.6 | 63.1 | 69.2 | 74.4 | **74.126** |
        - mlp + attentionë„ í•™ìŠµ
            
            
            | lr | code | law | math | medical | mmlu | average |
            | --- | --- | --- | --- | --- | --- | --- |
            | 2e-4 | 74.33 | 80.9 | 57.4 | 66.1 | 74.5 | **70.646** |
            | **2e-5** | 75 | 81.2 | 59.1 | 66.3 | 75.7 | **71.46** |
            | **2e-6** | 76 | 82.2 | 58.5 | 67.4 | 74.9 | **71.8** |

# MoE model

- moe-base model
    
    
    | domain | code | law | math | medical | mmlu | average | top-k |
    | --- | --- | --- | --- | --- | --- | --- | --- |
    | **eval-moe-base** | 69 | 77 | 53.6 | 67 | 74.2 | **68.16** | 1 |
    | **eval-moe-base** | 72 | 77.2 | 53.2 | 65.8 | 75.8 | 68.8 | 2 |
- D2H model
    
    domain modelë¥¼ ê²°í•©í•œ í˜•íƒœ
    
    - Routerë§Œ í•™ìŠµ
        
        
        | domain | code | law | math | medical | mmlu | average | top-k |
        | --- | --- | --- | --- | --- | --- | --- | --- |
        | **final_router_top1** | 78 | 90.1 | 66.3 | 68.3 | 75 | **75.54** | 1 |
        | **final_router_top2** | 75.67 | 86.2 | 65.1 | 68.7 | 76 | **74.334** | 2 |
    - ê°ì í•œë²ˆì— í•™ìŠµ
    
    | **domain** | **code** | **law** | **math** | **medical** | **mmlu** | **average** | **top-k** |
    | --- | --- | --- | --- | --- | --- | --- | --- |
    | **attention-noaux-5e5-top1** | 75.67 | 87.8 | 65.5 | 69.3 | 73.8 | **74.414** | 1 |
    | **mlp-attention-noaux-5e5-top1** | 72 | 88.4 | 60.8 | 64 | 73.3 | **71.7** | 1 |
    | **mlp-noaux-5e5-top1** | 75 | 88.1 | 63.8 | 66.8 | 73.4 | **73.42** | 1 |
    |  |  |  |  |  |  |  |  |
    | **attention-0-5e5-top2** | **76** | **87.1** | **63** | **68.1** | **73.6** | **73.56** | 2 |
    | **attention+mlp-noaux-5e5-top2** | **72.67** | **86.5** | **60.4** | **63.3** | **72.4** | **71.054** | 2 |
    | **mlp-noaux-5e5-top2** | **74.67** | **86.1** | **60.6** | **65.6** | **74.4** | **72.274** | 2 |
    - routerë¥¼ ë¨¼ì € í•™ìŠµí•œ í›„ ê²°í•©
        
        
        | domain | code | law | math | medical | mmlu | average | top-k |
        | --- | --- | --- | --- | --- | --- | --- | --- |
        | **trained_router_attention** | 78 | 90.1 | 66.3 | 68.2 | 74.5 | **75.42** | 1 |
        | **trained_router_mlp** | 78 | 90.2 | 66.4 | 68 | 74.7 | **75.46** | 1 |
        | **trained_router_attention+mlp** | 78 | 90.1 | 66.3 | 68.4 | 74.8 | **75.52** | 1 |
        | **trained_router_mlp** | 75.67 | 85.9 | 64.9 | 68.3 | 75.9 | **74.134** | 2 |
        | **trained_router_attention** | 76 | 86 | 64.7 | 68.7 | 75.8 | **74.24** | 2 |
        | **trained_router_attention+mlp** | 76 | 86.3 | 65.1 | 68.3 | 75.7 | **74.28** | 2 |
- zero adapter model
    - router+adapterë§Œ í•™ìŠµ
    
    | domain | code | law | math | medical | mmlu | average | top-k |
    | --- | --- | --- | --- | --- | --- | --- | --- |
    | **aux0_LR5e4** | 77.0 | 87.1 | 62.9 | 67.7 | 74.0 | 73.74 | 1 |
    | **aux0_LR5e4** | 76.67 | 88.5 | 66.9 | 69.9 | 73.4 | 75.074 | 2 |
    - router + adapter + attention í•™ìŠµ
    
    | domain | code | law | math | medical | mmlu | average | top-k |
    | --- | --- | --- | --- | --- | --- | --- | --- |
    | **noaux _lr5e5** | 77.0 | 84.4 | 61.5 | 66.9 | 75.2 | 73.0 | 1 |
    | **noaux_lr5e5** | 76.33 | 83.8 | 61.5 | 67.9 | 74.3 | 72.766 | 2 |



# Llama
# Dense model

- base model
    - í•™ìŠµí•˜ì§€ ì•Šì€ base model
        
        
        | domain | code | law | math | medical | mmlu | average |
        | --- | --- | --- | --- | --- | --- | --- |
        | eval-llama-base | 53 | 53.2 | 35.3 | 68.1 | 56.9 | 53.3 |
- domain model
    - ê° ë„ë©”ì¸ë³„ë¡œ í•™ìŠµí•œ ëª¨ë¸
        
        
        | domain | code | law | math | medical | mmlu | average |
        | --- | --- | --- | --- | --- | --- | --- |
        | code | 65.33 | 55.9 | 34.5 | 71 | 57.6 | 56.766 |
        | law | 47.33 | 88.2 | 33.9 | 62.2 | 51.3 | 56.586 |
        | math | 55.67 | 56.1 | 47.9 | 72.2 | 57.2 | 57.814 |
        | medical | 57.33 | 53.8 | 40 | 76.8 | 58 | 57.186 |
    - 20%ë§Œ ì‚¬ìš©í•œ ëª¨ë¸
        
        
        | domain | code | law | math | medical | mmlu | average |
        | --- | --- | --- | --- | --- | --- | --- |
        | **eval-code-only** | 59.67 | 55 | 35.8 | 72.7 | 56.9 | **56.014** |
        | **eval-math-only** | 56 | 55 | 38.8 | 72.9 | 57.4 | **56.02** |
        | **eval-medical-only** | 57.67 | 54.7 | 38.1 | 75.3 | 57.4 | **56.634** |
        | **eval-law-only** | 56 | 73.6 | 35.9 | 71.6 | 58.2 | **59.06** |
    
- general model
    
    ëª¨ë“  ë°ì´í„°ì…‹ì„ í•™ìŠµí•œ ëª¨ë¸
    
    - mlpë§Œ í•™ìŠµ
        
        
        | name | code | law | math | medical | mmlu | average |
        | --- | --- | --- | --- | --- | --- | --- |
        | **eval-adapter-general** | 62.67 | 79.9 | 44.5 | 76.3 | 57.7 | **64.214** |
    - mlp + attentionë„ í•™ìŠµ
        
        
        | lr | code | law | math | medical | mmlu | average |
        | --- | --- | --- | --- | --- | --- | --- |
        | 2e4 | 66.0 | 85.8 | 45.8 | 75.4 | 57.6 | **66.12** |

# MoE model

- moe-base model
    
    
    | domain | code | law | math | medical | mmlu | average | top-k |
    | --- | --- | --- | --- | --- | --- | --- | --- |
    | **eval-moe-base** | 58.33 | 73.6 | 37.1 | 73.2 | 57.7 | **59.986** | 1 |
    | **eval-moe-base** | 62.33 | 74.8 | 38.8 | 75 | 58.9 | **61.966** | 2 |
- D2H model
    
    domain modelë¥¼ ê²°í•©í•œ í˜•íƒœ
    
    - Routerë§Œ í•™ìŠµ
        
        
        | domain | code | law | math | medical | mmlu | average | top-k |
        | --- | --- | --- | --- | --- | --- | --- | --- |
        | **eval-epoch1_12600_noaux_5e4** | 63.33 | 87.6 | 46.3 | 72.4 | 57.9 | **65.506** | 1 |
        | **eval-epoch1_12600_noaux_2e4** | 65 | 85.4 | 46 | 73.4 | 58 | **65.56** | 2 |
    - ê°ì í•œë²ˆì— í•™ìŠµ
    
    | **domain** | **average** | **code** | **law** | **math** | **medical** | **mmlu** | **top-k** |
    | --- | --- | --- | --- | --- | --- | --- | --- |
    | **eval-attention-aux-0-lr-5e5-top1** | **64.66** | 65 | 86.6 | 45.7 | 68.3 | 57.7 | 1 |
    | **eval-attention+mlp-aux-0-lr-5e5-top1** | **65.18** | 62 | 87.3 | 46.2 | 72.4 | 58 | 1 |
    | **eval-mlp-aux-0-lr-5e5-top1** | **65.4** | 63 | 87.6 | 46.5 | 72.5 | 57.4 | 1 |
    | **eval-attention-aux-0-lr-5e5-top2** | **65.58** | 65 | 85.5 | 45.6 | 74 | 57.8 | 2 |
    | **eval-attention+mlp-aux-0-lr-5e5-top2** | **63** | 60 | 85.3 | 44.6 | 66.9 | 58.2 | 2 |
    | **eval-mlp-aux-0-lr-5e5-top2** | **65.506** | 65.33 | 85.6 | 45.1 | 73.5 | 58 | 2 |
    
    - routerë¥¼ ë¨¼ì € í•™ìŠµí•œ í›„ ê²°í•©
        
        
        | domain | average | code | law | math | medical | mmlu | top-k |
        | --- | --- | --- | --- | --- | --- | --- | --- |
        | **eval-trained-attention-aux-noaux-lr-2e5-top1** | **65.134** | 61.67 | 87.6 | 45.8 | 72.6 | 58 | 1 |
        | **eval-trained-attention+mlp-aux-noaux-lr-2e5-top1** | **65.186** | 63.33 | 88 | 44.8 | 72.5 | 57.3 | 1 |
        | **eval-trained-mlp-aux-noaux-lr-2e5-top1** | **65.306** | 63.33 | 87.9 | 45.3 | 72.3 | 57.7 | 1 |
        | **eval-trained-attention-aux-noaux-lr-2e5-top2** | **65.354** | 63.67 | 86.5 | 46.1 | 72.7 | 57.8 | 2 |
        | **eval-trained-attention+mlp-aux-noaux-lr-2e5-top2** | **65.374** | 63.67 | 87 | 45.9 | 72.2 | 58.1 | 2 |
        | **eval-trained-mlp-aux-noaux-lr-2e5-top2** | **65.074** | 63.67 | 86.5 | 45.3 | 72.3 | 57.6 | 2 |
- zero adapter model
    
    ì—¬ê¸°ë¥¼ ì¶”ê°€ë¡œ ì‹¤í—˜í•´í– í• ë“¯
    
    - router+adapterë§Œ í•™ìŠµ (0,1,2,6)ìœ¼ë¡œ ì“°ì(5e4)
    
    | domain | code | law | math | medical | mmlu | average | top-k |
    | --- | --- | --- | --- | --- | --- | --- | --- |
    | **zero-start-router_mlp** | 56 | 51.7 | 35.4 | 70.9 | 57 | **54.2** | 1 |
    | **zero-start-router_mlp_attention** | 61 | 81.6 | 42.5 | 71.3 | 57 | **62.68** | 2 |
    - router + adapter + attention í•™ìŠµ(5e5)
    
    | domain | code | law | math | medical | mmlu | average | top-k |
    | --- | --- | --- | --- | --- | --- | --- | --- |
    | **zero-start-router_mlp** | 56 | 51.7 | 35.4 | 70.9 | 57 | **54.2** | 1 |
    | **zero-start-router_mlp_attention** | 64.33 | 82.7 | 43.8 | 72.5 | 57.7 | **64.206** | 2 |



**Author**: Gyubin Choi    