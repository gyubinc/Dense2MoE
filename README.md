# Dense2MoE: Dense-to-MoE Transformation Pipeline

Dense ëª¨ë¸ì„ Mixture-of-Experts (MoE) ëª¨ë¸ë¡œ ë³€í™˜í•˜ëŠ” í†µí•© íŒŒì´í”„ë¼ì¸

## TL;DR

Dense LLM ëª¨ë¸ ê¸°ë°˜ ë„ë©”ì¸ë³„ LoRA ì–´ëŒ‘í„°ë¥¼ í›ˆë ¨í•˜ê³ , ì´ë¥¼ **Layer-wise MoE ì•„í‚¤í…ì²˜**ë¡œ í†µí•©í•˜ëŠ” End-to-End íŒŒì´í”„ë¼ì¸

### Main Features
- **Multi-Model ì§€ì›**: Llama, Qwen ë“± ë‹¤ì–‘í•œ Dense ëª¨ë¸ ì§€ì›
- **Layer-wise MoE**: ê° layerë§ˆë‹¤ ë…ë¦½ì ì¸ Router
- **Nê°œ Expert**: Nê°œ ë„ë©”ì¸ ì „ë¬¸ê°€ + 1ê°œ zero ì „ë¬¸ê°€
- **ìœ ì—°í•œ í•™ìŠµ**: Routerë§Œ í•™ìŠµ, Router+MLP í•™ìŠµ, Attention í•™ìŠµ ë“± ë‹¤ì–‘í•œ ì˜µì…˜
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: LoRAë¥¼ í†µí•œ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  êµ¬ì¡°

## MoE Architecture

### **Layer Structure**
```
Layer N:
â”œâ”€â”€ Self-Attention (freeze/trainable)
â”œâ”€â”€ Router N (trainable) â†’ N+1ê°œ ì¤‘ Top-K selection
â””â”€â”€ Expert Selection
    â”œâ”€â”€ Domain 1 Expert (LoRA MLP)
    â”œâ”€â”€ Domain 2 Expert (LoRA MLP)
    â”œâ”€â”€ ...
    â””â”€â”€ Zero Expert (ì›ë³¸ FFN)

Output = Î£(Router Weight Ã— Expert Output)
```

### **Overall Architecture**
- **Base Model**: ì„¤ì • ê°€ëŠ¥ (Llama, Qwen ë“±)
- **Experts per Layer**: N+1ê°œ (Nê°œ ë„ë©”ì¸ + 1ê°œ zero)
- **Gating Strategy**: Top-1 (Hard) / Top-2+ (Soft) ì„ íƒ ê°€ëŠ¥

## File Structure

```
Dense2MoE/
â”œâ”€â”€ src/                      # ğŸ—ï¸ í•µì‹¬ ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ models/               # ğŸ¤– MoE ëª¨ë¸ êµ¬í˜„ì²´
â”‚   â”‚   â””â”€â”€ model.py          # MoEModel, LayerRouter, ExpertFFN
â”‚   â”œâ”€â”€ core/                 # ğŸ”§ í•µì‹¬ ì»´í¬ë„ŒíŠ¸
â”‚   â”‚   â”œâ”€â”€ trainer.py        # ë„ë©”ì¸/ë¼ìš°í„° í›ˆë ¨ê¸°
â”‚   â”‚   â”œâ”€â”€ evaluator.py      # ë¼ìš°í„° í‰ê°€ê¸°
â”‚   â”‚   â””â”€â”€ dataset.py        # ë°ì´í„°ì…‹ ì²˜ë¦¬
â”‚   â””â”€â”€ utils/                # ğŸ› ï¸ ìœ í‹¸ë¦¬í‹°
â”‚       â”œâ”€â”€ utils.py          # GPU, ë¡œê¹…, í™˜ê²½ì„¤ì •
â”‚       â””â”€â”€ wandb_utils.py    # WandB ì—°ë™
â”œâ”€â”€ config/                   # âš™ï¸ ì„¤ì • ê´€ë¦¬
â”‚   â”œâ”€â”€ config.yaml           # ë©”ì¸ ì„¤ì • (model.typeìœ¼ë¡œ ëª¨ë¸ ì „í™˜)
â”‚   â”œâ”€â”€ moe.py                # MoE/ëª¨ë¸ ì„¤ì • (MODEL_REGISTRY)
â”‚   â””â”€â”€ domains.py            # ë„ë©”ì¸ ì„¤ì •
â”œâ”€â”€ scripts/                  # ğŸš€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ train_domain.py   # ë„ë©”ì¸ë³„ LoRA í•™ìŠµ
â”‚   â”‚   â””â”€â”€ train_router.py   # MoE ë¼ìš°í„° í•™ìŠµ
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ evaluate.py       # MoE í‰ê°€
â”‚       â””â”€â”€ evaluate_domain.py# ë„ë©”ì¸ LoRA í‰ê°€
â”œâ”€â”€ data/                     # ğŸ“Š ë°ì´í„°
â”œâ”€â”€ domain_models/            # ğŸ¯ í›ˆë ¨ëœ LoRA ì–´ëŒ‘í„°
â”œâ”€â”€ moe_models/               # ğŸ¤– í•™ìŠµëœ ë¼ìš°í„° ì²´í¬í¬ì¸íŠ¸
â””â”€â”€ requirements.txt          # ğŸ“¦ ì˜ì¡´ì„±
```

## ğŸ”§ Setting

### Model Switching (config.yaml)
```yaml
model:
  type: "llama"  # "llama" ë˜ëŠ” "qwen"
  name: null     # nullì´ë©´ typeì—ì„œ ìë™ ê²°ì •
```

### Supported Models (config/moe.py)
```python
MODEL_REGISTRY = {
    "llama": {"name": "meta-llama/Llama-3.2-3B-Instruct", "num_layers": 28},
    "qwen": {"name": "Qwen/Qwen3-4B-Instruct-2507", "num_layers": 36},
}
```

## Usage

### 1. Train Domain Lora Adapter

```bash
# ì˜ë£Œ ë„ë©”ì¸ í›ˆë ¨
python scripts/training/train_domain.py --domain medical --max-samples 1000

# ë²•ë¥  ë„ë©”ì¸ í›ˆë ¨
python scripts/training/train_domain.py --domain law --max-samples 1000
```

**Main arguments:**
- `--domain`: í•™ìŠµ ë„ë©”ì¸ (medical, law, math, code)
- `--max-samples`: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
- `--epochs`: ì—í­ ìˆ˜
- `--output-dir`: ì €ì¥ ê²½ë¡œ

### 2. Train MoE Router

```bash
# ë¼ìš°í„° í•™ìŠµ (Routerë§Œ)
python scripts/training/train_router.py --output-dir moe_models/run1 --target router

# ë¼ìš°í„° + MLP í•™ìŠµ
python scripts/training/train_router.py --output-dir moe_models/run1 --target router_mlp

# Attention í¬í•¨ í•™ìŠµ
python scripts/training/train_router.py --output-dir moe_models/run1 --target attention
```

**ì£¼ìš” ì˜µì…˜:**
- `--target`: í•™ìŠµ ëŒ€ìƒ (`router`, `mlp`, `attention`, `router_mlp`)
- `--top-k`: Expert ì„ íƒ ìˆ˜ (1: Hard, 2+: Soft routing)
- `--load-balancing-loss-weight`: ë¡œë“œ ë°¸ëŸ°ì‹± ì†ì‹¤ ê°€ì¤‘ì¹˜
- `--use-wandb`: WandB ë¡œê¹…

### 3. Evaluation

```bash
# MoE ëª¨ë¸ í‰ê°€
python scripts/evaluation/evaluate.py \
    --moe-model-path moe_models/run1/final_model/pytorch_model.bin \
    --domain medical --max-samples 200

# ë„ë©”ì¸ LoRA í‰ê°€
python scripts/evaluation/evaluate_domain.py --domain medical --max-samples 200
```

## Domain Datasets

| ë„ë©”ì¸ | ë°ì´í„°ì…‹ | ì„ íƒì§€ | Train | Test |
|--------|----------|--------|-------|------|
| Medical | MedMCQA | 4 | 20,000 | 1,000 |
| Law | casehold | 5 | 20,000 | 1,000 |
| Math | mathqa | 5 | 20,000 | 1,000 |
| Code | coding-mcq-reasoning | 4 | 3,000 | 300 |
| MMLU | MMLU | 4 | - | 1,000 |

## Pipeline

```bash
# 1. í™˜ê²½ í™œì„±í™”
conda activate moe

# 2. ëª¨ë“  ë„ë©”ì¸ LoRA í›ˆë ¨
for domain in medical law math code; do
    python scripts/training/train_domain.py --domain $domain --epochs 2
done

# 3. MoE ë¼ìš°í„° í›ˆë ¨
python scripts/training/train_router.py --output-dir moe_models/run1 --epochs 1

# 4. í‰ê°€
python scripts/evaluation/evaluate.py \
    --moe-model-path moe_models/run1/final_model/pytorch_model.bin \
    --domain medical
```

## Results directory

- `domain_models/<domain>/`: ë„ë©”ì¸ë³„ LoRA ì–´ëŒ‘í„°
- `moe_models/<run>/final_model/`: í•™ìŠµëœ ë¼ìš°í„°
- `*_training_summary.json`: í•™ìŠµ ìš”ì•½

---

**Author**: Gyubin Choi
